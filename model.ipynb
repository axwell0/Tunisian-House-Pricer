{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:25.271971Z",
     "start_time": "2024-11-25T03:54:22.417624Z"
    }
   },
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode_top_n_cities(df, city_column, n):\n",
    "    h = df.copy()\n",
    "    top_cities = h[city_column].value_counts().nlargest(n).index\n",
    "\n",
    "    h[city_column] = h[city_column].apply(lambda x: x if x in top_cities else 'Other')\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    h[f'{city_column}_encoded'] = label_encoder.fit_transform(h[city_column])\n",
    "\n",
    "    return h\n",
    "\n",
    "def train_xgb_with_hyperopt(train_set, test_set, numerical_features, target_column, categorical_features=None, max_evals=100):\n",
    "    \"\"\"\n",
    "    Train an XGBoost model with hyperparameter tuning using hyperopt.\n",
    "\n",
    "    Parameters:\n",
    "        train_set (pd.DataFrame): Pre-separated training set.\n",
    "        test_set (pd.DataFrame): Pre-separated testing set.\n",
    "        numerical_features (list): List of numerical feature column names.\n",
    "        target_column (str): Target column name.\n",
    "        categorical_features (list, optional): List of categorical feature column names. Default is None.\n",
    "        max_evals (int): Maximum number of hyperparameter optimization iterations. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the best hyperparameters and evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = train_set[numerical_features + categorical_features] if categorical_features else train_set[numerical_features]\n",
    "    y_train = train_set[target_column]\n",
    "\n",
    "    X_test = test_set[numerical_features + categorical_features] if categorical_features else test_set[numerical_features]\n",
    "    y_test = test_set[target_column]\n",
    "    def objective(params):\n",
    "        params['max_depth'] = int(params['max_depth'])\n",
    "        params['min_child_weight'] = int(params['min_child_weight'])\n",
    "        model = xgb.XGBRegressor(enable_categorical=True, **params)\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        r2_scores = []\n",
    "\n",
    "        for train_idx, val_idx in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_val_pred = model.predict(X_val_fold)\n",
    "            r2 = r2_score(y_val_fold, y_val_pred)\n",
    "            r2_scores.append(r2)\n",
    "\n",
    "        mean_r2 = np.mean(r2_scores)\n",
    "        return {'loss': -mean_r2, 'status': STATUS_OK}\n",
    "\n",
    "    space = {\n",
    "        'n_estimators': hp.choice('n_estimators', [100, 200, 300]),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.09, 0.5),\n",
    "        'max_depth': hp.choice('max_depth', [3, 5, 7, 9, 12]),\n",
    "        'min_child_weight': hp.choice('min_child_weight', [1, 3, 5, 7]),\n",
    "        'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': hp.uniform('gamma', 0, 0.4),\n",
    "        'reg_alpha': hp.uniform('reg_alpha', 0, 0.05),\n",
    "        'reg_lambda': hp.uniform('reg_lambda', 0, 0.05),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': 42\n",
    "    }\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    best_params = space_eval(space, best)\n",
    "\n",
    "    final_model = xgb.XGBRegressor(enable_categorical=True, **best_params)\n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_test_pred = final_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    r2_scores, rmse_scores, mae_scores = [], [], []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model = xgb.XGBRegressor(enable_categorical=True, **best_params)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        y_val_pred = model.predict(X_val_fold)\n",
    "        r2_scores.append(r2_score(y_val_fold, y_val_pred))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_val_fold, y_val_pred)))\n",
    "        mae_scores.append(mean_absolute_error(y_val_fold, y_val_pred))\n",
    "\n",
    "    return {\n",
    "        'final_model':final_model,\n",
    "        'best_params': best_params,\n",
    "        'test_rmse': rmse,\n",
    "        'test_mae': mae,\n",
    "        'test_r2': r2,\n",
    "        'cv_mean_r2': np.mean(r2_scores),\n",
    "        'cv_mean_rmse': np.mean(rmse_scores),\n",
    "        'cv_mean_mae': np.mean(mae_scores)\n",
    "    }\n",
    "\n",
    "\n",
    "def remove_outliers_iqr(df, column,quantiles=(0.25,0.75) ):\n",
    "    Q1 = df[column].quantile(quantiles[0])\n",
    "    Q3 = df[column].quantile(quantiles[1])\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    df_no_outliers = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "    print(f\"IQR Method: Removed {df.shape[0] - df_no_outliers.shape[0]} rows.\")\n",
    "    return df_no_outliers\n",
    "\n",
    "def get_collection(client, name):\n",
    "    return pd.DataFrame(client['Houses'][name].find({})).drop(['_id', 'title', 'url','description'], axis=1)\n",
    "def convert_price_to_tnd(df, price_column='price', eur_to_tnd=3.34):\n",
    "    h = df.copy()\n",
    "    h[price_column] = (\n",
    "        h[price_column]\n",
    "        .str.extract(r'(\\d[\\d\\s]*)')[0]\n",
    "        .str.replace(r'\\s+', '', regex=True).str.replace(',','',regex=True)\n",
    "        .astype('Int64')\n",
    "        * h[price_column].apply(lambda x: eur_to_tnd if 'EUR' in x else 1)\n",
    "    )\n",
    "    h = h.reset_index(drop=True)\n",
    "    return h.copy()\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:25.398871Z",
     "start_time": "2024-11-25T03:54:25.273002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import warnings\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv('secret.env')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "mongodb_uri = os.getenv('MONGODB_URI')\n",
    "client = pymongo.MongoClient(mongodb_uri)\n",
    "\n"
   ],
   "id": "c6914a83c60238e8",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3> Affare data pre-processing",
   "id": "2772bf2bd8379005"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:30.674404Z",
     "start_time": "2024-11-25T03:54:25.688871Z"
    }
   },
   "cell_type": "code",
   "source": "affare = get_collection(client, 'Affare')",
   "id": "a4605e5ef33213cd",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:30.803107Z",
     "start_time": "2024-11-25T03:54:30.675378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "affare = affare[affare['price'].isna() == False]\n",
    "affare = convert_price_to_tnd(affare, 'price')\n",
    "affare.drop(['Meubl√©e','posting_date','Adresse'],axis=1,inplace=True)\n",
    "affare['Chambre'] = affare['Chambre'].str.extract('(\\d+)').astype('Int64')\n",
    "affare['Salles de bains'] = affare['Salles de bains'].str.extract('(\\d+)').astype('Int64')\n",
    "affare['Superficie'] = affare['Superficie'].str.extract('(\\d+)').astype('Int64')\n",
    "affare['city'] = affare['location'].str.split(' - ', expand=True).loc[:, 1].str.lower()\n",
    "affare['state'] = affare['location'].str.split(' - ', expand=True).loc[:, 0].str.lower()\n",
    "affare['Type'] = affare['Type'].fillna('villa')\n",
    "affare.rename(\n",
    "    {\"Chambre\": 'n_bedrooms',\n",
    "    'Salles de bains': 'n_bathrooms',\n",
    "    'Superficie': 'area'}\n",
    "    , axis='columns', inplace=True)\n",
    "affare['city'] = affare['city'].apply(lambda x: 'hammamet' if 'hammamet' in x else x)\n",
    "affare = affare[(affare['area'].isna() == False) & (affare['n_bathrooms'].isna() == False)]\n"
   ],
   "id": "20fc9e9b157545f0",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3> Menzili pre-processing",
   "id": "4859e323f3fcace2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:43.635831Z",
     "start_time": "2024-11-25T03:54:30.804120Z"
    }
   },
   "cell_type": "code",
   "source": "menzili = get_collection(client, 'menzili')",
   "id": "15e6b6a61aabfa35",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:43.692160Z",
     "start_time": "2024-11-25T03:54:43.636896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "menzili['price'] = menzili['price'].str.replace(' ', '').str.extract('(\\d+)').astype('Int64')\n",
    "menzili.dropna(subset=['Surf terrain', 'Salle de bain', 'Chambres', 'price', 'location'],inplace=True)\n",
    "menzili.drop(['Pi√©ces Totale', 'Ann√©e construction', 'Surf habitable', 'misc'], axis=1, inplace=True)\n",
    "menzili.rename({'Chambres': 'n_bedrooms', 'Salle de bain': 'n_bathrooms', 'Surf terrain': 'area'}, axis='columns', inplace=True)\n",
    "menzili['n_bedrooms'] = menzili['n_bedrooms'].str.replace('+', '').astype('Int64')\n",
    "menzili['n_bathrooms'] = menzili['n_bathrooms'].str.replace('+', '').astype('Int64')\n",
    "menzili['area'] = menzili['area'].str.extract('(\\d+)').astype('Int64')\n",
    "menzili['state'] = menzili['location'].str.split(', ', expand=True).loc[:, 2].str.replace('√©', 'e').str.lower()\n",
    "menzili['city'] = menzili['location'].str.split(', ', expand=True).loc[:, 1].str.replace('√©', 'e').str.lower()\n",
    "menzili.dropna(subset=['city', 'state'], inplace=True)\n",
    "menzili['city'] = menzili['city'].str.replace('djerba - midoun', 'djerba').apply(lambda x: 'hammamet' if 'hammamet' in x else x)\n"
   ],
   "id": "f4df62ce42984664",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Filtering the aggregated dataset. We will only consider houses in the price range of 80,000 - 1,100,000 TND.\n",
    "\n",
    "Only houses with Area < 1500m¬≤ will be considered due to data imbalance "
   ],
   "id": "47d00df3136b8f40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:55:12.252552Z",
     "start_time": "2024-11-25T03:55:12.220947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aggregate_df = pd.concat([affare,menzili])\n",
    "aggregate_df = aggregate_df[(aggregate_df['price'] > 80000) & (aggregate_df['price'] <= 1000000)]\n",
    "aggregate_df = aggregate_df[(aggregate_df['n_bedrooms'] <= 7) & (aggregate_df['n_bedrooms'] >= 1)]\n",
    "aggregate_df = aggregate_df[(aggregate_df['n_bathrooms'] >= 1) & (aggregate_df['n_bathrooms'] < 7)]\n",
    "aggregate_df = aggregate_df[(aggregate_df['area'] >= 100) & (aggregate_df['area'] <= 1500)]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "aggregate_df['type_encoded'] = label_encoder.fit_transform(aggregate_df['Type'])\n",
    "aggregate_df['price_log'] = np.log1p(aggregate_df['price'])\n",
    "\n",
    "\n",
    "aggregate_df['area_log'] = np.log1p(aggregate_df['area'])"
   ],
   "id": "8b3fb1425cbb5672",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:57:09.442339Z",
     "start_time": "2024-11-25T03:55:34.819463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "train_set, test_set = train_test_split(aggregate_df, test_size=0.2, random_state=42)\n",
    "\n",
    "def preprocess_data(train_df):\n",
    "    temp = train_df.copy()\n",
    "    # # \n",
    "    # temp = remove_outliers_iqr(temp, 'area')\n",
    "    # temp = remove_outliers_iqr(temp, 'n_bedrooms')\n",
    "    # Encode top N cities and states\n",
    "    temp = encode_top_n_cities(temp, 'city', 30)\n",
    "    temp = encode_top_n_cities(temp, 'state', 10)\n",
    "    temp['city'] = temp['city'].astype('category')\n",
    "    temp['state'] = temp['state'].astype('category')\n",
    "    return temp\n",
    "\n",
    "def preprocss_test_data(test_set, cities,cities_encoding,states,states_encoding):\n",
    "    temp = test_set.copy()\n",
    "    temp['city'] = temp['city'].apply(lambda x: x if x in cities.value_counts() else 'Other')\n",
    "    temp['city'] = temp['city'].astype('category')\n",
    "    t = pd.concat([cities,cities_encoding],axis=1).drop_duplicates()\n",
    "\n",
    "    temp['city_encoded'] = temp['city'].apply(lambda x: t.loc[t['city']==x,'city_encoded'].iloc[0])\n",
    "    temp['state'] = temp['state'].apply(lambda x: x if x in states.value_counts() else 'Other')\n",
    "    temp['state'] = temp['state'].astype('category')\n",
    "    t = pd.concat([states,states_encoding],axis=1).drop_duplicates()\n",
    "\n",
    "    temp['state_encoded'] = temp['state'].apply(lambda x: t.loc[t['state']==x,'state_encoded'].iloc[0])\n",
    "    return temp\n",
    "    \n",
    "    \n",
    "\n",
    "train_set_preprocessed = preprocess_data(train_set)\n",
    "test_set_preprocessed = preprocss_test_data(test_set, train_set_preprocessed['city'],train_set_preprocessed['city_encoded'],train_set_preprocessed['state'],train_set_preprocessed['state_encoded'])\n",
    "# \n",
    "FEATURES = ['area', 'n_bedrooms', 'n_bathrooms','city_encoded','state_encoded','type_encoded']\n",
    "model_results = train_xgb_with_hyperopt(train_set_preprocessed, test_set_preprocessed, FEATURES, 'price')\n"
   ],
   "id": "625a8f03138ef800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:32<00:00,  1.08trial/s, best loss: -0.7772433638572693]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:57:09.449469Z",
     "start_time": "2024-11-25T03:57:09.443339Z"
    }
   },
   "cell_type": "code",
   "source": "model_results",
   "id": "87e6b6a7bb03e672",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'final_model': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7334342542663905, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=True,\n",
       "              eval_metric='rmse', feature_types=None, gamma=0.048799306537731214,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.0999968145747301,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=3, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...),\n",
       " 'best_params': {'colsample_bytree': 0.7334342542663905,\n",
       "  'eval_metric': 'rmse',\n",
       "  'gamma': 0.048799306537731214,\n",
       "  'learning_rate': 0.0999968145747301,\n",
       "  'max_depth': 7,\n",
       "  'min_child_weight': 3,\n",
       "  'n_estimators': 300,\n",
       "  'objective': 'reg:squarederror',\n",
       "  'reg_alpha': 0.049587270692457074,\n",
       "  'reg_lambda': 0.0011110648558167735,\n",
       "  'seed': 42,\n",
       "  'subsample': 0.6936017987000839},\n",
       " 'test_rmse': 126773.04185687916,\n",
       " 'test_mae': 84491.49451639401,\n",
       " 'test_r2': 0.7714487910270691,\n",
       " 'cv_mean_r2': 0.7772433638572693,\n",
       " 'cv_mean_rmse': 125629.39835854925,\n",
       " 'cv_mean_mae': 83070.25282416618}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:18.329455Z",
     "start_time": "2024-11-25T03:54:18.327448Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d09695409f410c43",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:18.350137Z",
     "start_time": "2024-11-25T03:54:18.347719Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a47130ce8de7ff6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:18.368659Z",
     "start_time": "2024-11-25T03:54:18.366144Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "18448862a5e18bdb",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:12.374699Z",
     "start_time": "2024-11-25T03:54:12.374699Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a1bdeea9743037d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a0c9b5e7d4e26a2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T03:54:12.377699Z",
     "start_time": "2024-11-25T03:54:12.377699Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9def39b881d315d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "36eccabee2ba5f1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "25dfb30c46cef72a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
